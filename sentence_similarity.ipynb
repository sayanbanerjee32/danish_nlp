{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danish Sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('D:/DataScienceWorkSpace/danish_sentence_similarty/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data fie path in local\n",
    "data_file = '../data/sentences.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-12-176</td>\n",
       "      <td>Vanddamp er en usynlig gas, der forekommer i s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-13-182</td>\n",
       "      <td>Er der nogen herinde der har erfaring med at k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-14-29</td>\n",
       "      <td>Ved ikke lige hvordan disse er i størrelsen?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-16-39</td>\n",
       "      <td>Dog kan jeg godt lide pang farver;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-17-297</td>\n",
       "      <td>Pengene bliver dog ofte først udbetalt efter 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text\n",
       "0  10-12-176  Vanddamp er en usynlig gas, der forekommer i s...\n",
       "1  10-13-182  Er der nogen herinde der har erfaring med at k...\n",
       "2   10-14-29       Ved ikke lige hvordan disse er i størrelsen?\n",
       "3   10-16-39                Dog kan jeg godt lide pang farver;)\n",
       "4  10-17-297  Pengene bliver dog ofte først udbetalt efter 5..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the sentences in a dataframe\n",
    "sentence_df = pd.read_csv(data_file)\n",
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ifølge Dansk Kennelklub angriber muskelhunde dyr og mennesker cirka hver 14. dag.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target sentence for finding similar senences\n",
    "target_sentence = sentence_df.loc[sentence_df['id']=='7-21-440','text']\n",
    "target_sentence.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English translation for this sentence - __According to the Danish Kennel Club, muscular dogs attack animals and humans approximately every 14 days.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Toenization > cleaning > Count vectorizer / TF-IDF vectorizer > Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.snowball import DanishStemmer\n",
    "\n",
    "\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iflg',\n",
       " 'dansk',\n",
       " 'kennelklub',\n",
       " 'angrib',\n",
       " 'muskelhund',\n",
       " 'dyr',\n",
       " 'og',\n",
       " 'mennesk',\n",
       " 'cirka',\n",
       " 'hver',\n",
       " 'dag']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization function with Danish language Stemmer\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(DanishStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "# clean text by lowering abd removing email, number, url, currency, punctuation, \n",
    "def normalize_text(text):\n",
    "    return clean(text,\n",
    "     no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "    no_numbers=True,               # replace all numbers with a special token\n",
    "    no_digits=True,                # replace all digits with a special token\n",
    "    no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "    no_punct=True,\n",
    "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"\",\n",
    "    replace_with_email=\"\",\n",
    "    replace_with_phone_number=\"\",\n",
    "    replace_with_number=\"\",\n",
    "    replace_with_currency_symbol=\"\",\n",
    "#     replace_with_punct=\" ~PUNCT~ \",          # instead of removing punctuations you may replace them\n",
    "#     replace_with_url=\" ~URL~ \",\n",
    "#     replace_with_email=\" ~EMAIL~ \",\n",
    "#     replace_with_phone_number=\" ~PHONE~ \",\n",
    "#     replace_with_number=\" ~NUMBER~ \",\n",
    "#     replace_with_currency_symbol=\" ~CUR~ \",\n",
    "    lang=\"de\") # german is used as assumed that german would be closeer to Danish that english\n",
    "\n",
    "# sample output for the target text\n",
    "tokenize(normalize_text(target_sentence.values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       vanddamp er en usynlig gas der forekommer i st...\n",
       "1       er der nogen herinde der har erfaring med at k...\n",
       "2              ved ikke lige hvordan disse er i strrelsen\n",
       "3                       dog kan jeg godt lide pang farver\n",
       "4       pengene bliver dog ofte frst udbetalt efter da...\n",
       "                              ...                        \n",
       "4995    i dag i idrt skulle vi sa have bip test hvor j...\n",
       "4996    p men tnkt nu hvis de bragte mere fra danskspr...\n",
       "4997                             kathani hejsa allesammen\n",
       "4998    weeeeeee jeg har mega optur pa de var begge mi...\n",
       "4999    archon der blev jo ikke sagt noget om hvem der...\n",
       "Name: text, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a replica of the sentence dataframe for processing\n",
    "sentence_df_cp_op1 = sentence_df.copy()\n",
    "sentence_df_cp_op1 = sentence_df_cp_op1.text.apply(normalize_text)\n",
    "sentence_df_cp_op1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity on count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 13574)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit count vectorizer with Danish stop words\n",
    "# use the custom tokenizer with Danish Stemmer\n",
    "count_vect = CountVectorizer(tokenizer=tokenize, stop_words=stopwords.words('danish'))\n",
    "# fit on cleaned text\n",
    "count_vect_matrix = count_vect.fit_transform(sentence_df_cp_op1)\n",
    "# vocab sze 13574\n",
    "count_vect_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cosine similarity for the count vector matrix\n",
    "cosine_matrix_cv = cosine_similarity(count_vect_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Data Frame\n",
    "cosine_df_cv = pd.DataFrame(cosine_matrix_cv, columns = sentence_df['id'], index = sentence_df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "7-21-440      1.000000\n",
       "47-2-28       0.316228\n",
       "51-33-2622    0.258199\n",
       "36-0-1231     0.239046\n",
       "42-26-238     0.239046\n",
       "6-37-308      0.223607\n",
       "45-86-368     0.223607\n",
       "38-8-2425     0.223607\n",
       "51-89-2877    0.223607\n",
       "20-10-28      0.223607\n",
       "Name: 7-21-440, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the row for target sentence\n",
    "target_cosine_array = cosine_df_cv.loc['7-21-440',:]\n",
    "# sort descending\n",
    "target_cosine_array.sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather indices for top 10 similar sentences\n",
    "top_10_similarity = target_cosine_array.sort_values(ascending = False)[:10].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I dag'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[1],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hver dag i december op til juleaften sendes et afsnit.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[2],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Det er der min arbejdsinteresse ligger, og det er noget, som jeg kan bruge hver dag.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[3],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Danskerne er blandt de europæere, som hver for sig producerer mest affald.'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[4],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hovedårsagen til, at Katrina blev så dyr, at skaderne blev så omfangsrige, er ganske enkelt, at der i dag bor langt flere mennesker i kystområderne end tidligere.'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[5],'text'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity on TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\danish_nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['all', 'and', 'bliv', 'dis', 'eft', 'ell', 'hav', 'havd', 'hend', 'ikk', 'kun', 'mang', 'meg', 'nog', 'nogl', 'skul', 'und', 'vær'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 13574)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit tf-idf vectorizer with Danish stop words\n",
    "# use the custom tokenizer with Danish Stemmer\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tokenize, stop_words=stopwords.words('danish'))\n",
    "# fit on cleaned text\n",
    "tfidf_vect_matrix = tfidf_vect.fit_transform(sentence_df_cp_op1)\n",
    "tfidf_vect_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise cosine similarity for tf-idf vector matrix\n",
    "cosine_matrix_ti = cosine_similarity(tfidf_vect_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DF\n",
    "cosine_df_ti = pd.DataFrame(cosine_matrix_ti, columns = sentence_df['id'], index = sentence_df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "7-21-440      1.000000\n",
       "47-2-28       0.216134\n",
       "38-20-1059    0.180219\n",
       "45-86-368     0.179803\n",
       "36-0-1231     0.174177\n",
       "38-78-1469    0.167071\n",
       "34-2-723      0.166653\n",
       "6-37-308      0.166108\n",
       "40-81-2158    0.154190\n",
       "51-33-2622    0.152718\n",
       "Name: 7-21-440, dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select row for target sentence\n",
    "target_cosine_array = cosine_df_ti.loc['7-21-440',:]\n",
    "# sort descending\n",
    "target_cosine_array.sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for top 10 similr sentences\n",
    "top_10_similarity = target_cosine_array.sort_values(ascending = False)[:10].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I dag'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[1],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jeg ved, at niveauet er højt blandt alle angriberne i Serie A og kræver en toppræstation hver eneste gang.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[2],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hvad er dyrets yndlingsfoder?'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[3],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Det er der min arbejdsinteresse ligger, og det er noget, som jeg kan bruge hver dag.'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[4],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Men det fik ikke den farlige angriber til at skåne City.'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[5],'text'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: spaCy word embedding vector for sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"da_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10-12-176</th>\n",
       "      <td>vanddamp er en usynlig gas der forekommer i st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10-13-182</th>\n",
       "      <td>er der nogen herinde der har erfaring med at k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10-14-29</th>\n",
       "      <td>ved ikke lige hvordan disse er i strrelsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10-16-39</th>\n",
       "      <td>dog kan jeg godt lide pang farver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10-17-297</th>\n",
       "      <td>pengene bliver dog ofte frst udbetalt efter da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        text\n",
       "id                                                          \n",
       "10-12-176  vanddamp er en usynlig gas der forekommer i st...\n",
       "10-13-182  er der nogen herinde der har erfaring med at k...\n",
       "10-14-29          ved ikke lige hvordan disse er i strrelsen\n",
       "10-16-39                   dog kan jeg godt lide pang farver\n",
       "10-17-297  pengene bliver dog ofte frst udbetalt efter da..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create replica for cleaned text df from last section\n",
    "sentence_df_cp_op2 = sentence_df_cp_op1.copy()\n",
    "sentence_df_cp_op2 = pd.DataFrame(sentence_df_cp_op2.values, columns = ['text'], index = sentence_df['id'])\n",
    "sentence_df_cp_op2.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp pipeline for target sentence\n",
    "base = nlp(sentence_df_cp_op2.loc['7-21-440',:].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function for similarity scoring between 2 sentences\n",
    "def calculate_similarity(text2):\n",
    "    compare = nlp(text2)\n",
    "    return base.similarity(compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\danish_nlp\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# apply on each row\n",
    "sentence_df_cp_op2['sim_score'] = sentence_df_cp_op2['text'].apply(calculate_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort descending\n",
    "sentence_df_cp_op2.sort_values(by = ['sim_score'], ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 similar sentences, index 0 being the sentence itself\n",
    "top_10 = sentence_df_cp_op2.iloc[:10,:]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nar man kommer slbende med vrkende kn og ildrd nse efter timers vandring gr det godt at stte sig til bordet sammen med de andre gster og bliver krset for i en fransk bjerghytte'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stadig flere danskere fatter interesse for den kontante sportsgren hvor veltrnede mnd pa langt over kilo kaster sig efter hinanden'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'det tager i hvert fald en hel dag at overvre indmarchen af de snorlige rkker af musketerer farverige flagkastere grnne skytter krigere med armbrster byens skyttedronning og de forskellige madvogne som helt bogstaveligt smider brd og plser i hovedet pa publikum'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'og nok stter to unge mnd sig pa fortovscafeen og far en kop kaffe og taler arabisk men det er cafe latte de taler ogsa dansk og de har parkeret en stor kassevogn foran cafeen'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hver lrdag klokken mdes en gruppe kinesiske forldre og deres brn i little mermaid chinese culture school der startede som et privat initiativ i mens forldrene sludrer og dyrker tai chi far brnene undervisning i kinesisk sprog og kultur i de lante klasselokaler'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: BERT embedding for Danish > Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT based sentence embedding for Danish\n",
    "from danish_bert_embeddings import DanishBertEmbeddings\n",
    "embedder = DanishBertEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample embedding\n",
    "embedding = embedder.embed(target_sentence.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to embedding for each sentence\n",
    "sentence_df_embed = sentence_df['text'].apply(embedder.embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to np array\n",
    "sentence_embed_list = [t.numpy() for t in sentence_df_embed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairewwise cosine similarity between sentence embeddings\n",
    "cosine_matrix = cosine_similarity(sentence_embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "cosine_df = pd.DataFrame(cosine_matrix, columns = sentence_df['id'], index = sentence_df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "7-21-440      1.000000\n",
       "38-21-615     0.713109\n",
       "36-0-2615     0.705893\n",
       "6-37-308      0.705858\n",
       "51-3-3235     0.699858\n",
       "36-7-877      0.694900\n",
       "40-44-510     0.693219\n",
       "3-28-67       0.692790\n",
       "38-59-1673    0.692642\n",
       "51-34-3098    0.692097\n",
       "Name: 7-21-440, dtype: float32"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the row for target sentence\n",
    "target_cosine_array = cosine_df.loc['7-21-440',:]\n",
    "# sort descending\n",
    "target_cosine_array.sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather indices for top 10 similr sentences\n",
    "top_10_similarity = target_cosine_array.sort_values(ascending = False)[:10].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Han er fossilekspert, og foruden at være museumsinspektør ved Geomuseum Faxe er han forsker ved Københavns Universitet:\" Over revet var der 200- 400 meter havvand, og Thoracosaurus har- akkurat som nulevende havkrokodiller- jagtet sit bytte i de øvre vandlag.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[1],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'På 24 timer mellem 15. og 16. april 1949 fløj 1. 398 maskiner i alt 12. 849 tons fragt ind til den isolerede storby'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[2],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hovedårsagen til, at Katrina blev så dyr, at skaderne blev så omfangsrige, er ganske enkelt, at der i dag bor langt flere mennesker i kystområderne end tidligere.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[3],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Men indtil nu er alle teorierne kommet mere eller mindre til kort, da man hele tiden kan finde dyr, der har noget, der ligner menneskelige egenskaber.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[4],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Børshandlere er såvidt vides udstyret med samme biologiske profil som alle andre mennesker.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[5],'text'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 4: Sub word tokenization > TF-IDF vercorization > cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file for subword tokenization training\n",
    "sub_word_file = '../data/sub_word_training.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = '\\n'.join(sentence_df['text'])\n",
    "with open(sub_word_file, 'w') as _file:\n",
    "    _file.write(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train sentencepiece model from sub_word_file and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train('--input=../data/sub_word_training.txt --model_prefix=m --vocab_size=10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁If', 'ø', 'lge', '▁Dansk', '▁Kenne', 'l', 'klub', '▁angriber', '▁mu', 'skel', 'hund', 'e', '▁dyr', '▁og', '▁mennesker', '▁cirk', 'a', '▁hver', '▁14', '.', '▁dag', '.']\n",
      "[930, 0, 184, 363, 2899, 59, 3358, 3042, 4114, 8993, 4280, 16, 1092, 6, 335, 1428, 86, 190, 527, 5, 155, 5]\n"
     ]
    }
   ],
   "source": [
    "# encode: text => id\n",
    "print(sp.encode_as_pieces(target_sentence.values[0]))\n",
    "print(sp.encode_as_ids(target_sentence.values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity on TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a replica of the sentence dataframe for processing\n",
    "sentence_df_cp_op4 = sentence_df.copy()\n",
    "sentence_df_cp_op4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 8095)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit count vectorizer with Danish stop words\n",
    "# use the custom tokenizer with Danish Stemmer\n",
    "tfidf_vect_sw = TfidfVectorizer(tokenizer=sp.encode_as_ids)#, stop_words=stopwords.words('danish'))\n",
    "# fit on cleaned text\n",
    "tfidf_vect_matrix_sw = tfidf_vect_sw.fit_transform(sentence_df_cp_op4.text)\n",
    "# vocab sze 13574\n",
    "tfidf_vect_matrix_sw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cosine similarity for the count vector matrix\n",
    "cosine_matrix_ti_sw = cosine_similarity(tfidf_vect_matrix_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Data Frame\n",
    "cosine_df_ti_sw = pd.DataFrame(cosine_matrix_ti_sw, columns = sentence_df['id'], index = sentence_df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "7-21-440      1.000000\n",
       "34-7-768      0.186515\n",
       "47-2-28       0.175104\n",
       "40-81-2158    0.170774\n",
       "38-78-1469    0.169889\n",
       "34-60-944     0.167158\n",
       "38-20-1059    0.163503\n",
       "51-33-1121    0.163082\n",
       "6-37-308      0.154364\n",
       "36-90-2811    0.149406\n",
       "Name: 7-21-440, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the row for target sentence\n",
    "target_cosine_array = cosine_df_ti_sw.loc['7-21-440',:]\n",
    "# sort descending\n",
    "target_cosine_array.sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather indices for top 10 similar sentences\n",
    "top_10_similarity = target_cosine_array.sort_values(ascending = False)[:10].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hvor forbrugernes valgmuligheder for 20 år siden kunne tælles på en hånd eller to, så kan de i dag vælge mellem cirka 150 forskellige realkreditlån.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[1],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I dag'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[2],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ifølge Henning Otte Hansen må forbrugerne herhjemme nu nok under alle omstændigheder affinde sig med, at maden er relativt dyr:'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[3],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Men det fik ikke den farlige angriber til at skåne City.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[4],'text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ifølge kilderne er brødrene rejst udenlands.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.loc[sentence_df.id == top_10_similarity[5],'text'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 5: Sub word tokenization > TOP2VEC for topic clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\danish_nlp\\lib\\site-packages\\umap\\__init__.py:9: UserWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "  warn(\"Tensorflow not installed; ParametricUMAP will be unavailable\")\n"
     ]
    }
   ],
   "source": [
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "universal-sentence-encoder-multilingual is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-f0e48cba8221>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtopic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTop2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_df_cp_op4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'universal-sentence-encoder-multilingual'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspeed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"deep-learn\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\danish_nlp\\lib\\site-packages\\top2vec\\Top2Vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, min_count, embedding_model, embedding_model_path, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, verbose)\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_import_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Pre-processing documents for training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\danish_nlp\\lib\\site-packages\\top2vec\\Top2Vec.py\u001b[0m in \u001b[0;36m_check_import_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'distiluse-base-multilingual-cased'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_HAVE_TENSORFLOW\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m                 raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n\u001b[0m\u001b[0;32m    767\u001b[0m                                   \u001b[1;34m\"Try: pip install top2vec[sentence_encoders]\\n\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m                                   \"Alternatively try: pip install tensorflow tensorflow_hub tensorflow_text\")\n",
      "\u001b[1;31mImportError\u001b[0m: universal-sentence-encoder-multilingual is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text"
     ]
    }
   ],
   "source": [
    "topic_model = Top2Vec(list(sentence_df_cp_op4.text),embedding_model='universal-sentence-encoder',speed=\"deep-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4118,  882], dtype=int64), array([0, 1], dtype=int64))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sizes, topic_nums = topic_model.get_topic_sizes()\n",
    "topic_sizes, topic_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
